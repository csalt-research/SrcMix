# SrcMix: Mixing of Related Source Languages Benefits Extremely Low-resource Machine Translation

ğŸ“¢ **Accepted to EACL 2026 (Findings)**

Official implementation of **SrcMix**, a simple and effective
training-time strategy for improving machine translation in **Extremely
Low-Resource Languages (ELRLs)**.

------------------------------------------------------------------------

## Motivation

Extremely low-resource languages (ELRLs) often suffer from:

-   Fewer than 6K parallel sentences
-   Limited supervision
-   High typological diversity
-   Weak transfer from massively multilingual models

Standard multilingual training (naÃ¯ve concatenation or many-to-many setups) Often performs poorly in ELRL settings due to:

-   Negative transfer
-   Cross-lingual interference
-   Fragmented decoder supervision

**SrcMix** addresses this by introducing multilinguality only on the
*source side*, while keeping the decoder specialized to a single target
ELRL.

------------------------------------------------------------------------

## What is SrcMix?

Let:

-   Target ELRL: n1
-   Related source languages: n2, n3, ..., nN
-   High-resource language: H

To train H â†’ n1, we mix:

H â†’ n1
n2 â†’ n1
n3 â†’ n1
...
nN â†’ n1

Key design choices:

-   Remove explicit source language identifiers
-   Force implicit structural transfer
-   Preserve decoder specialization

Unlike naÃ¯ve multilingual mixing, SrcMix prevents decoder fragmentation
and improves performance under extreme data scarcity.

------------------------------------------------------------------------

## ğŸ§¡ First Public Angika MT Dataset

This work introduces the **first publicly available machine translation
training and evaluation dataset for Angika**, an Indo-Aryan language
spoken in eastern India.

Dataset available here:

https://huggingface.co/datasets/snjev310/AngikaMT

The dataset includes:

-   Parallel training data
-   Evaluation splits
-   FLORES-aligned dev/test sets
-   Clean preprocessing for reproducibility

------------------------------------------------------------------------

## ğŸ“‚ Repository Structure

```text
srcmix/
â”‚
â”œâ”€â”€ srcmix/
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_single.py
â”‚   â”‚   â”œâ”€â”€ train_srcmix.py
â”‚   â”‚   â””â”€â”€ train_naive_mix.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ build_dataset.py
â”‚   â”‚   â”œâ”€â”€ mixing.py
â”‚   â”‚   â””â”€â”€ splits.py
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ evaluate.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ config.py
â”‚
â”œâ”€â”€ configs/
â”œâ”€â”€ scripts/
â””â”€â”€ README.md
```

------------------------------------------------------------------------

## Training

### Single Supervised Training

bash scripts/run_single.sh

### SrcMix Training

bash scripts/run_srcmix.sh

### NaÃ¯ve Multilingual Mixing (Baseline)

bash scripts/run_ablations.sh

------------------------------------------------------------------------

## Evaluation

python srcmix/evaluation/evaluate.py --peft_model_path path_to_adapter
--test_data path_to_tokenized_test

Metrics:

-   BLEU (sacreBLEU)
-   ChrF++ 

------------------------------------------------------------------------

## ğŸ”¬ Experimental Setup

-   Base Model: Aya-101 (CohereForAI), mT5-Large
-   Fine-tuning: LoRA (r=16)
-   Training data: \~6K sentences per language
-   Evaluation: FLORES-aligned splits
-   Metrics: BLEU and ChrF++

------------------------------------------------------------------------

## Key Contributions

-   Introduce **SrcMix**, a source-side mixing strategy tailored for
    ELRLs.
-   Show that naÃ¯ve multilingual mixing underperforms in extreme data
    scarcity.
-   Release the first MT training & evaluation dataset for Angika.
-   Provide systematic comparison across language families and scripts.
-   Demonstrate consistent gains across multiple ELRLs.

------------------------------------------------------------------------

## ğŸ“œ Citation

If you use this dataset or the associated research in your work, please cite it as follows:

```bibtex
We will provide the final BibTeX entry once the paper is publicly available through the EACL proceedings.

```
------------------------------------------------------------------------

## ğŸ¤ Acknowledgements

This work was supported by a Ph.D.Â grant from the TCS Research Foundation and the Amazon-IIT Bombay AI/ML initiative.

------------------------------------------------------------------------

## ğŸ“¬ Contact

**Sanjeev Kumar** CSE IIT Bombay  
Email: `sanjeev@cse.iitb.ac.in`
